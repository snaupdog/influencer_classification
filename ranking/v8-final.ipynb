{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383b303c",
   "metadata": {
    "papermill": {
     "duration": 0.004893,
     "end_time": "2025-11-23T13:03:21.734698",
     "exception": false,
     "start_time": "2025-11-23T13:03:21.729805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# InfluencerRank v8: Best Single Model (GAT + GRU)\n",
    "\n",
    "**Model**: Heterogeneous GNN with GAT convolution + GRU\n",
    "\n",
    "**Key Features**:\n",
    "1. HeteroGNN with GAT layers\n",
    "2. Layer concatenation: [F^(1), F^(2)] = 256 dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b162879",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:03:21.744157Z",
     "iopub.status.busy": "2025-11-23T13:03:21.743911Z",
     "iopub.status.idle": "2025-11-23T13:06:42.245863Z",
     "shell.execute_reply": "2025-11-23T13:06:42.245145Z"
    },
    "papermill": {
     "duration": 200.507899,
     "end_time": "2025-11-23T13:06:42.247203",
     "exception": false,
     "start_time": "2025-11-23T13:03:21.739304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.6.0+cu124\n",
      "Uninstalling torch-2.6.0+cu124:\n",
      "  Successfully uninstalled torch-2.6.0+cu124\n",
      "Found existing installation: torchvision 0.21.0+cu124\n",
      "Uninstalling torchvision-0.21.0+cu124:\n",
      "  Successfully uninstalled torchvision-0.21.0+cu124\n",
      "Found existing installation: torchaudio 2.6.0+cu124\n",
      "Uninstalling torchaudio-2.6.0+cu124:\n",
      "  Successfully uninstalled torchaudio-2.6.0+cu124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torch-geometric as it is not installed.\n",
      "WARNING: Skipping torch-scatter as it is not installed.\n",
      "WARNING: Skipping torch-sparse as it is not installed.\n",
      "WARNING: Skipping torch-cluster as it is not installed.\n",
      "WARNING: Skipping torch-spline-conv as it is not installed.\n",
      "WARNING: Skipping pyg-lib as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\r\n",
      "Collecting torch==2.3.1+cu121\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp311-cp311-linux_x86_64.whl (781.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torchvision==0.18.1+cu121\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torchaudio==2.3.1+cu121\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (3.20.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (4.15.0)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (1.13.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.1+cu121) (2025.10.0)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting triton==2.3.1 (from torch==2.3.1+cu121)\r\n",
      "  Downloading https://download.pytorch.org/whl/triton-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.18.1+cu121) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.18.1+cu121) (11.3.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1+cu121) (12.5.82)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.1+cu121) (3.0.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.1+cu121) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.1+cu121) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.1+cu121) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.1+cu121) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.1+cu121) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.1+cu121) (2.4.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.1+cu121) (1.3.0)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.18.1+cu121) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.18.1+cu121) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.18.1+cu121) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.18.1+cu121) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.18.1+cu121) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.18.1+cu121) (2024.2.0)\r\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchaudio, torchvision\r\n",
      "  Attempting uninstall: triton\r\n",
      "    Found existing installation: triton 3.2.0\r\n",
      "    Uninstalling triton-3.2.0:\r\n",
      "      Successfully uninstalled triton-3.2.0\r\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\r\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\r\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\r\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\r\n",
      "  Attempting uninstall: nvidia-nccl-cu12\r\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\r\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\r\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.1+cu121 torchaudio-2.3.1+cu121 torchvision-0.18.1+cu121 triton-2.3.1\r\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\r\n",
      "Collecting torch-scatter==2.1.2+pt23cu121\r\n",
      "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcu121/torch_scatter-2.1.2%2Bpt23cu121-cp311-cp311-linux_x86_64.whl (10.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torch-sparse==0.6.18+pt23cu121\r\n",
      "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcu121/torch_sparse-0.6.18%2Bpt23cu121-cp311-cp311-linux_x86_64.whl (5.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torch-cluster==1.6.3+pt23cu121\r\n",
      "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcu121/torch_cluster-1.6.3%2Bpt23cu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torch-spline-conv==1.2.2+pt23cu121\r\n",
      "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt23cu121-cp311-cp311-linux_x86_64.whl (950 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.4/950.4 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse==0.6.18+pt23cu121) (1.15.3)\r\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse==0.6.18+pt23cu121) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18+pt23cu121) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18+pt23cu121) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18+pt23cu121) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18+pt23cu121) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18+pt23cu121) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18+pt23cu121) (2.4.1)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18+pt23cu121) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18+pt23cu121) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18+pt23cu121) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18+pt23cu121) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18+pt23cu121) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy->torch-sparse==0.6.18+pt23cu121) (2024.2.0)\r\n",
      "Installing collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster\r\n",
      "Successfully installed torch-cluster-1.6.3+pt23cu121 torch-scatter-2.1.2+pt23cu121 torch-sparse-0.6.18+pt23cu121 torch-spline-conv-1.2.2+pt23cu121\r\n",
      "Collecting torch-geometric\r\n",
      "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.13.2)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.10.0)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.1.3)\r\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.0.9)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.6.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.8.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.7.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.4.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.22.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2.4.1)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.10.5)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric) (2024.2.0)\r\n",
      "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: torch-geometric\r\n",
      "Successfully installed torch-geometric-2.7.0\r\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\r\n",
      "Collecting pyg-lib\r\n",
      "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcu121/pyg_lib-0.4.0%2Bpt23cu121-cp311-cp311-linux_x86_64.whl (2.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pyg-lib\r\n",
      "Successfully installed pyg-lib-0.4.0+pt23cu121\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.system('pip uninstall torch torchvision torchaudio torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv pyg-lib -y')\n",
    "\n",
    "!pip install torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "!pip install torch-scatter==2.1.2+pt23cu121 torch-sparse==0.6.18+pt23cu121 torch-cluster==1.6.3+pt23cu121 torch-spline-conv==1.2.2+pt23cu121 -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
    "\n",
    "!pip install torch-geometric\n",
    "!pip install pyg-lib -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
    "\n",
    "import os\n",
    "# os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff812ab9",
   "metadata": {
    "papermill": {
     "duration": 0.038795,
     "end_time": "2025-11-23T13:06:42.325365",
     "exception": false,
     "start_time": "2025-11-23T13:06:42.286570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a69d182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:06:42.404854Z",
     "iopub.status.busy": "2025-11-23T13:06:42.404593Z",
     "iopub.status.idle": "2025-11-23T13:06:58.340409Z",
     "shell.execute_reply": "2025-11-23T13:06:58.339534Z"
    },
    "papermill": {
     "duration": 15.977314,
     "end_time": "2025-11-23T13:06:58.341629",
     "exception": false,
     "start_time": "2025-11-23T13:06:42.364315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ndcg_score\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch_geometric.nn import SAGEConv, GATConv, HeteroConv\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb236a5d",
   "metadata": {
    "papermill": {
     "duration": 0.040738,
     "end_time": "2025-11-23T13:06:58.422507",
     "exception": false,
     "start_time": "2025-11-23T13:06:58.381769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac808e69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:06:58.504056Z",
     "iopub.status.busy": "2025-11-23T13:06:58.503306Z",
     "iopub.status.idle": "2025-11-23T13:06:58.509025Z",
     "shell.execute_reply": "2025-11-23T13:06:58.508296Z"
    },
    "papermill": {
     "duration": 0.047355,
     "end_time": "2025-11-23T13:06:58.510008",
     "exception": false,
     "start_time": "2025-11-23T13:06:58.462653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ V8 Best Model (GAT+GRU)\n",
      "✅ Masking 17 features\n"
     ]
    }
   ],
   "source": [
    "# V3 GRAPHS (with actual edges!)\n",
    "# GRAPH_DIR = 'graphs_enhanced_v3'  # Local\n",
    "GRAPH_DIR = '/kaggle/input/graphsv3/graphs_enhanced_v3'  # Kaggle\n",
    "\n",
    "# STRICT MASKING (17 features)\n",
    "STRICT_MASK_INDICES = [\n",
    "    11, 12, 13, 14, 15, 16, 17, 18, 19, 20,  # Temporal Trends (10)\n",
    "    21, 22,                                  # Activity Rate & Consistency\n",
    "    23, 24,                                  # Growth Features\n",
    "    26, 27,                                  # Raw Monthly Counts\n",
    "    36                                       # Sentiment Rate\n",
    "]\n",
    "\n",
    "print(f\"✅ V8 Best Model (GAT+GRU)\")\n",
    "print(f\"✅ Masking {len(STRICT_MASK_INDICES)} features\")\n",
    "\n",
    "# Architecture\n",
    "INPUT_DIM = 37\n",
    "GNN_HIDDEN = 128\n",
    "GNN_OUT = 128\n",
    "RNN_HIDDEN = 128\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 64\n",
    "LIST_SIZE = 10\n",
    "LEARNING_RATE = 0.0015\n",
    "NUM_EPOCHS = 50\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# Temporal\n",
    "TRAINING_MONTHS = 9\n",
    "TARGET_MONTH = 9\n",
    "SEED = 44\n",
    "\n",
    "# Split\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "# Save directory\n",
    "MODEL_SAVE_DIR = 'saved_models_v8_final'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89cd08b",
   "metadata": {
    "papermill": {
     "duration": 0.039364,
     "end_time": "2025-11-23T13:06:58.589971",
     "exception": false,
     "start_time": "2025-11-23T13:06:58.550607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Load V3 Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7b692b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:06:58.669700Z",
     "iopub.status.busy": "2025-11-23T13:06:58.669040Z",
     "iopub.status.idle": "2025-11-23T13:07:02.193235Z",
     "shell.execute_reply": "2025-11-23T13:07:02.192341Z"
    },
    "papermill": {
     "duration": 3.565072,
     "end_time": "2025-11-23T13:07:02.194432",
     "exception": false,
     "start_time": "2025-11-23T13:06:58.629360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading V3 graphs...\n",
      "  JAN: 2556 inf, 34139 hashtag, 17924 user, 80 obj, 210050 edges\n",
      "  FEB: 2745 inf, 35493 hashtag, 19035 user, 80 obj, 224801 edges\n",
      "  MAR: 2998 inf, 42116 hashtag, 24285 user, 80 obj, 289724 edges\n",
      "  APR: 3258 inf, 46382 hashtag, 26540 user, 80 obj, 318269 edges\n",
      "  MAY: 3549 inf, 52662 hashtag, 30100 user, 80 obj, 374002 edges\n",
      "  JUN: 3831 inf, 56844 hashtag, 31715 user, 80 obj, 399204 edges\n",
      "  JUL: 4136 inf, 62930 hashtag, 35677 user, 80 obj, 471521 edges\n",
      "  AUG: 4465 inf, 70364 hashtag, 39673 user, 80 obj, 548174 edges\n",
      "  SEP: 4808 inf, 74746 hashtag, 42989 user, 80 obj, 582115 edges\n",
      "  OCT: 5180 inf, 84229 hashtag, 47669 user, 80 obj, 683653 edges\n",
      "  NOV: 5545 inf, 89226 hashtag, 51248 user, 80 obj, 733528 edges\n",
      "  DEC: 5973 inf, 97018 hashtag, 55221 user, 80 obj, 840515 edges\n",
      "\n",
      "✅ Loaded 12 v3 heterogeneous graphs\n"
     ]
    }
   ],
   "source": [
    "month_names = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \n",
    "               \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "\n",
    "print(\"Loading V3 graphs...\")\n",
    "all_graphs_data = []\n",
    "\n",
    "for month_idx, month in enumerate(month_names):\n",
    "    path = os.path.join(GRAPH_DIR, f\"{month}_graph.pt\")\n",
    "    data = torch.load(path, weights_only=False)\n",
    "    all_graphs_data.append(data)\n",
    "    \n",
    "    graph = data['graph']\n",
    "    total_edges = sum(graph[edge_type].edge_index.shape[1] for edge_type in graph.edge_types)\n",
    "    \n",
    "    print(f\"  {month.upper()}: {graph['influencer'].num_nodes} inf, \"\n",
    "          f\"{graph['hashtag'].num_nodes} hashtag, {graph['user'].num_nodes} user, \"\n",
    "          f\"{graph['object'].num_nodes} obj, {total_edges} edges\")\n",
    "\n",
    "print(f\"\\n✅ Loaded {len(all_graphs_data)} v3 heterogeneous graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1127990b",
   "metadata": {
    "papermill": {
     "duration": 0.040822,
     "end_time": "2025-11-23T13:07:02.275349",
     "exception": false,
     "start_time": "2025-11-23T13:07:02.234527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8041d5cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:07:02.354904Z",
     "iopub.status.busy": "2025-11-23T13:07:02.354647Z",
     "iopub.status.idle": "2025-11-23T13:07:02.360716Z",
     "shell.execute_reply": "2025-11-23T13:07:02.360067Z"
    },
    "papermill": {
     "duration": 0.047318,
     "end_time": "2025-11-23T13:07:02.361698",
     "exception": false,
     "start_time": "2025-11-23T13:07:02.314380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: Train=4144, Val=518, Test=518\n"
     ]
    }
   ],
   "source": [
    "target_data = all_graphs_data[TARGET_MONTH]\n",
    "all_influencers = list(target_data['maps']['influencer'].keys())\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(all_influencers)\n",
    "\n",
    "n = len(all_influencers)\n",
    "n_train = int(TRAIN_RATIO * n)\n",
    "n_val = int(VAL_RATIO * n)\n",
    "\n",
    "train_influencers = set(all_influencers[:n_train])\n",
    "val_influencers = set(all_influencers[n_train:n_train + n_val])\n",
    "test_influencers = set(all_influencers[n_train + n_val:])\n",
    "\n",
    "print(f\"Split: Train={len(train_influencers)}, Val={len(val_influencers)}, Test={len(test_influencers)}\")\n",
    "\n",
    "train_influencers_list = list(train_influencers)\n",
    "val_influencers_list = list(val_influencers)\n",
    "test_influencers_list = list(test_influencers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b157e5d",
   "metadata": {
    "papermill": {
     "duration": 0.040049,
     "end_time": "2025-11-23T13:07:02.441035",
     "exception": false,
     "start_time": "2025-11-23T13:07:02.400986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Train-Only Normalization and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92649a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:07:02.521827Z",
     "iopub.status.busy": "2025-11-23T13:07:02.521593Z",
     "iopub.status.idle": "2025-11-23T13:07:02.775697Z",
     "shell.execute_reply": "2025-11-23T13:07:02.774917Z"
    },
    "papermill": {
     "duration": 0.296335,
     "end_time": "2025-11-23T13:07:02.777008",
     "exception": false,
     "start_time": "2025-11-23T13:07:02.480673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-only normalization and STRICT leakage masking...\n",
      "Masking 17 features to force RNN learning.\n",
      "Indices: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 36]\n",
      "✅ Normalized and masked. Model must learn from structure and content.\n"
     ]
    }
   ],
   "source": [
    "print(\"Train-only normalization and STRICT leakage masking...\")\n",
    "\n",
    "# 1. Collect training features for scaler fitting\n",
    "train_features = []\n",
    "for month_idx in range(TRAINING_MONTHS):\n",
    "    features = all_graphs_data[month_idx]['graph']['influencer'].x\n",
    "    influencer_map = all_graphs_data[month_idx]['maps']['influencer']\n",
    "    for inf_name in train_influencers:\n",
    "        if inf_name in influencer_map:\n",
    "            train_features.append(features[influencer_map[inf_name]].numpy())\n",
    "\n",
    "# 2. Fit Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.vstack(train_features))\n",
    "\n",
    "print(f\"Masking {len(STRICT_MASK_INDICES)} features to force RNN learning.\")\n",
    "print(f\"Indices: {STRICT_MASK_INDICES}\")\n",
    "\n",
    "# 3. Apply Normalization AND Masking\n",
    "for data_package in all_graphs_data:\n",
    "    features = data_package['graph']['influencer'].x\n",
    "    normalized_features = torch.FloatTensor(scaler.transform(features.numpy()))\n",
    "    \n",
    "    # APPLY MASK\n",
    "    normalized_features[:, STRICT_MASK_INDICES] = 0.0\n",
    "    \n",
    "    data_package['graph']['influencer'].x = normalized_features\n",
    "\n",
    "print(f\"✅ Normalized and masked. Model must learn from structure and content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053472f1",
   "metadata": {
    "papermill": {
     "duration": 0.112275,
     "end_time": "2025-11-23T13:07:02.930951",
     "exception": false,
     "start_time": "2025-11-23T13:07:02.818676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Prepare Heterogeneous Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c6e213e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:07:03.013371Z",
     "iopub.status.busy": "2025-11-23T13:07:03.012651Z",
     "iopub.status.idle": "2025-11-23T13:07:03.246267Z",
     "shell.execute_reply": "2025-11-23T13:07:03.245342Z"
    },
    "papermill": {
     "duration": 0.276938,
     "end_time": "2025-11-23T13:07:03.247961",
     "exception": false,
     "start_time": "2025-11-23T13:07:02.971023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing heterogeneous graphs for GPU...\n",
      "  Month 0: 420100 edges (undirected)\n",
      "  Month 1: 449602 edges (undirected)\n",
      "  Month 2: 579448 edges (undirected)\n",
      "  Month 3: 636538 edges (undirected)\n",
      "  Month 4: 748004 edges (undirected)\n",
      "  Month 5: 798408 edges (undirected)\n",
      "  Month 6: 943042 edges (undirected)\n",
      "  Month 7: 1096348 edges (undirected)\n",
      "  Month 8: 1164230 edges (undirected)\n",
      "\n",
      "✅ All 9 heterogeneous graphs prepared on GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPreparing heterogeneous graphs for GPU...\")\n",
    "hetero_graphs = []\n",
    "\n",
    "for month_idx in range(TRAINING_MONTHS):\n",
    "    graph = all_graphs_data[month_idx]['graph']\n",
    "    graph = T.ToUndirected()(graph)\n",
    "    graph = graph.to(device)\n",
    "    \n",
    "    influencer_map = all_graphs_data[month_idx]['maps']['influencer']\n",
    "    \n",
    "    hetero_graphs.append({\n",
    "        'graph': graph,\n",
    "        'influencer_map': influencer_map\n",
    "    })\n",
    "    \n",
    "    total_edges = sum(graph[edge_type].edge_index.shape[1] for edge_type in graph.edge_types)\n",
    "    print(f\"  Month {month_idx}: {total_edges} edges (undirected)\")\n",
    "\n",
    "print(f\"\\n✅ All {len(hetero_graphs)} heterogeneous graphs prepared on GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c066a78",
   "metadata": {
    "papermill": {
     "duration": 0.042086,
     "end_time": "2025-11-23T13:07:03.334459",
     "exception": false,
     "start_time": "2025-11-23T13:07:03.292373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fe36d92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:07:03.416093Z",
     "iopub.status.busy": "2025-11-23T13:07:03.415811Z",
     "iopub.status.idle": "2025-11-23T13:07:03.431353Z",
     "shell.execute_reply": "2025-11-23T13:07:03.430568Z"
    },
    "papermill": {
     "duration": 0.058329,
     "end_time": "2025-11-23T13:07:03.432524",
     "exception": false,
     "start_time": "2025-11-23T13:07:03.374195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model architecture defined (GAT + GRU)\n"
     ]
    }
   ],
   "source": [
    "class HeteroGNN(nn.Module):\n",
    "    \"\"\"Heterogeneous GNN with GAT convolution.\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, metadata, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv1 = HeteroConv({\n",
    "            edge_type: GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=1)\n",
    "            for edge_type in metadata[1]\n",
    "        }, aggr='sum')\n",
    "        \n",
    "        self.conv2 = HeteroConv({\n",
    "            edge_type: GATConv((-1, -1), out_channels, add_self_loops=False, heads=1)\n",
    "            for edge_type in metadata[1]\n",
    "        }, aggr='sum')\n",
    "        \n",
    "        self.linear_proj = nn.Linear(in_channels, hidden_channels + out_channels)\n",
    "        \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        edge_index_dict_filtered = {k: v for k, v in edge_index_dict.items() if v.shape[1] > 0}\n",
    "        if len(edge_index_dict_filtered) == 0:\n",
    "            return {key: self.linear_proj(x) for key, x in x_dict.items()}\n",
    "\n",
    "        # Layer 1\n",
    "        x_dict_1 = self.conv1(x_dict, edge_index_dict_filtered)\n",
    "        x_dict_1 = {key: F.relu(x) for key, x in x_dict_1.items() if x is not None}\n",
    "        x_dict_1 = {key: self.dropout(x) for key, x in x_dict_1.items()}\n",
    "        \n",
    "        if len(x_dict_1) == 0:\n",
    "            return {key: self.linear_proj(x) for key, x in x_dict.items()}\n",
    "\n",
    "        # Layer 2\n",
    "        edge_index_dict_filtered_2 = {k: v for k, v in edge_index_dict_filtered.items() if k[0] in x_dict_1 and k[2] in x_dict_1}\n",
    "        if len(edge_index_dict_filtered_2) == 0:\n",
    "            x_dict_2 = {k: torch.zeros_like(v) for k, v in x_dict_1.items()}\n",
    "        else:\n",
    "            x_dict_2 = self.conv2(x_dict_1, edge_index_dict_filtered_2)\n",
    "            x_dict_2 = {key: x for key, x in x_dict_2.items() if x is not None}\n",
    "\n",
    "        return {k: torch.cat([x_dict_1[k], x_dict_2[k]], dim=1) for k in x_dict_1.keys() if k in x_dict_2}\n",
    "\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention_fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, hidden_states, lengths):\n",
    "        batch_size, seq_len, _ = hidden_states.shape\n",
    "        scores = self.attention_fc(hidden_states).squeeze(-1)\n",
    "        mask = torch.arange(seq_len, device=hidden_states.device).expand(batch_size, -1)\n",
    "        mask = mask < lengths.unsqueeze(1)\n",
    "        scores = scores.masked_fill(~mask, -1e9)\n",
    "        weights = F.softmax(scores, dim=1)\n",
    "        return torch.bmm(weights.unsqueeze(1), hidden_states).squeeze(1)\n",
    "\n",
    "\n",
    "class BestInfluencerModel(nn.Module):\n",
    "    \"\"\"Best performing model: GAT + GRU.\"\"\"\n",
    "    def __init__(self, input_dim, gnn_hidden, gnn_out, rnn_hidden, metadata, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hetero_gnn = HeteroGNN(input_dim, gnn_hidden, gnn_out, metadata, dropout)\n",
    "        \n",
    "        gnn_concat_dim = gnn_hidden + gnn_out\n",
    "        self.rnn = nn.GRU(gnn_concat_dim, rnn_hidden, num_layers=1, batch_first=True)\n",
    "        self.attention = SimpleAttention(rnn_hidden)\n",
    "        self.fc1 = nn.Linear(rnn_hidden, rnn_hidden // 2)\n",
    "        self.fc2 = nn.Linear(rnn_hidden // 2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def encode_graphs(self, hetero_graphs):\n",
    "        embeddings = []\n",
    "        for hg in hetero_graphs:\n",
    "            graph = hg['graph']\n",
    "            x_dict = {node_type: graph[node_type].x for node_type in graph.node_types if hasattr(graph[node_type], 'x')}\n",
    "            edge_index_dict = graph.edge_index_dict if len(graph.edge_index_dict) > 0 else {}\n",
    "            emb_dict = self.hetero_gnn(x_dict, edge_index_dict)\n",
    "            embeddings.append(emb_dict['influencer'])\n",
    "        return embeddings\n",
    "    \n",
    "    def forward_temporal(self, sequences, lengths):\n",
    "        packed = pack_padded_sequence(sequences, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        rnn_output, _ = pad_packed_sequence(self.rnn(packed)[0], batch_first=True)\n",
    "        context = self.attention(rnn_output, lengths.to(rnn_output.device))\n",
    "        x = F.relu(self.fc1(context))\n",
    "        return self.fc2(self.dropout(x)).squeeze(-1)\n",
    "\n",
    "print(\"✅ Model architecture defined (GAT + GRU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3ca70",
   "metadata": {
    "papermill": {
     "duration": 0.039975,
     "end_time": "2025-11-23T13:07:03.513371",
     "exception": false,
     "start_time": "2025-11-23T13:07:03.473396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Loss & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae7b1344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:07:03.598497Z",
     "iopub.status.busy": "2025-11-23T13:07:03.597854Z",
     "iopub.status.idle": "2025-11-23T13:07:03.605759Z",
     "shell.execute_reply": "2025-11-23T13:07:03.605022Z"
    },
    "papermill": {
     "duration": 0.052587,
     "end_time": "2025-11-23T13:07:03.606807",
     "exception": false,
     "start_time": "2025-11-23T13:07:03.554220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loss & metrics defined\n"
     ]
    }
   ],
   "source": [
    "def listwise_ranking_loss(y_pred, y_true):\n",
    "    \"\"\"Listwise ranking loss.\"\"\"\n",
    "    pred_diff = y_pred.unsqueeze(1) - y_pred.unsqueeze(0)\n",
    "    true_diff = y_true.unsqueeze(1) - y_true.unsqueeze(0)\n",
    "    mask = (true_diff > 0).float()\n",
    "    return (-F.logsigmoid(pred_diff) * mask).sum() / mask.sum().clamp(min=1)\n",
    "\n",
    "def compute_ndcg_at_k(y_true, y_pred, k=50):\n",
    "    if len(y_true) < 2: return 0.0\n",
    "    return ndcg_score(np.array(y_true).reshape(1, -1), np.array(y_pred).reshape(1, -1), k=min(k, len(y_true)))\n",
    "\n",
    "def get_ground_truth(influencer_names, target_data):\n",
    "    ground_truth = []\n",
    "    influencer_map = target_data['maps']['influencer']\n",
    "    engagement_rates = target_data['ground_truth']['engagement_rate']\n",
    "    for name in influencer_names:\n",
    "        ground_truth.append(engagement_rates[influencer_map[name]].item() if name in influencer_map else 0.0)\n",
    "    return torch.FloatTensor(ground_truth)\n",
    "\n",
    "def sample_influencers(influencer_list, size):\n",
    "    return [influencer_list[i] for i in np.random.choice(len(influencer_list), size, replace=False)] if len(influencer_list) >= size else influencer_list\n",
    "\n",
    "print(\"✅ Loss & metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e985ea3",
   "metadata": {
    "papermill": {
     "duration": 0.042278,
     "end_time": "2025-11-23T13:07:03.692184",
     "exception": false,
     "start_time": "2025-11-23T13:07:03.649906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ababb72e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:07:03.799184Z",
     "iopub.status.busy": "2025-11-23T13:07:03.798887Z",
     "iopub.status.idle": "2025-11-23T13:07:03.816528Z",
     "shell.execute_reply": "2025-11-23T13:07:03.815504Z"
    },
    "papermill": {
     "duration": 0.081571,
     "end_time": "2025-11-23T13:07:03.817815",
     "exception": false,
     "start_time": "2025-11-23T13:07:03.736244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training loop defined\n"
     ]
    }
   ],
   "source": [
    "def train_single_model(hetero_graphs, train_list, val_list, test_list, all_graphs_data, seed=SEED):\n",
    "    print(f\"\\n{'='*60}\\nTraining Best Model (GAT + GRU) with seed {seed}\\n{'='*60}\")\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    metadata = hetero_graphs[0]['graph'].metadata()\n",
    "    target_data = all_graphs_data[TARGET_MONTH]\n",
    "    \n",
    "    model = BestInfluencerModel(\n",
    "        INPUT_DIM, GNN_HIDDEN, GNN_OUT, RNN_HIDDEN, metadata, DROPOUT\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize model with dummy forward pass to avoid lazy parameter issues\n",
    "    print(\"Initializing model...\")\n",
    "    model.train()\n",
    "    _ = model.encode_graphs(hetero_graphs)\n",
    "    print(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    best_val_ndcg = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        \n",
    "        num_batches = max(1, len(train_list) // (BATCH_SIZE * LIST_SIZE))\n",
    "        for _ in range(num_batches):\n",
    "            optimizer.zero_grad()\n",
    "            embeddings = model.encode_graphs(hetero_graphs)\n",
    "            \n",
    "            batch_names = sample_influencers(train_list, LIST_SIZE * BATCH_SIZE)\n",
    "            sequences, valid_names = [], []\n",
    "            for name in batch_names:\n",
    "                seq = []\n",
    "                for m in range(TRAINING_MONTHS):\n",
    "                    if name in hetero_graphs[m]['influencer_map']:\n",
    "                        seq.append(embeddings[m][hetero_graphs[m]['influencer_map'][name]])\n",
    "                if len(seq) > 0:\n",
    "                    sequences.append(torch.stack(seq))\n",
    "                    valid_names.append(name)\n",
    "            \n",
    "            if len(sequences) < 2: continue\n",
    "            lengths = torch.LongTensor([s.shape[0] for s in sequences])\n",
    "            padded = pad_sequence(sequences, batch_first=True)\n",
    "            y_true = get_ground_truth(valid_names, target_data).to(device)\n",
    "            y_pred = model.forward_temporal(padded, lengths)\n",
    "            \n",
    "            loss = listwise_ranking_loss(y_pred, y_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del embeddings\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_emb = model.encode_graphs(hetero_graphs)\n",
    "            val_seqs, val_names = [], []\n",
    "            for name in val_list:\n",
    "                seq = [val_emb[m][hetero_graphs[m]['influencer_map'][name]] for m in range(TRAINING_MONTHS) if name in hetero_graphs[m]['influencer_map']]\n",
    "                if len(seq)>0: val_seqs.append(torch.stack(seq)); val_names.append(name)\n",
    "            \n",
    "            if len(val_seqs) > 0:\n",
    "                val_pad = pad_sequence(val_seqs, batch_first=True)\n",
    "                val_len = torch.LongTensor([s.shape[0] for s in val_seqs])\n",
    "                val_pred = model.forward_temporal(val_pad, val_len).cpu().numpy()\n",
    "                val_true = get_ground_truth(val_names, target_data).numpy()\n",
    "                val_ndcg = compute_ndcg_at_k(val_true, val_pred, k=50)\n",
    "                \n",
    "                if val_ndcg > best_val_ndcg:\n",
    "                    best_val_ndcg = val_ndcg\n",
    "                    best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        \n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print(f\"  Epoch {epoch+1}: Val NDCG={val_ndcg:.4f}\")\n",
    "\n",
    "    # Load Best & Test\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_emb = model.encode_graphs(hetero_graphs)\n",
    "        test_seqs, test_names = [], []\n",
    "        for name in test_list:\n",
    "            seq = [test_emb[m][hetero_graphs[m]['influencer_map'][name]] for m in range(TRAINING_MONTHS) if name in hetero_graphs[m]['influencer_map']]\n",
    "            if len(seq)>0: test_seqs.append(torch.stack(seq)); test_names.append(name)\n",
    "        \n",
    "        test_pad = pad_sequence(test_seqs, batch_first=True)\n",
    "        test_len = torch.LongTensor([s.shape[0] for s in test_seqs])\n",
    "        test_pred = model.forward_temporal(test_pad, test_len).cpu().numpy()\n",
    "        test_true = get_ground_truth(test_names, target_data).numpy()\n",
    "        test_ndcg = compute_ndcg_at_k(test_true, test_pred, k=50)\n",
    "        \n",
    "        print(f\"\\n  🏁 Best Val NDCG: {best_val_ndcg:.4f}\")\n",
    "        print(f\"  🎯 Test NDCG@50: {test_ndcg:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'best_val_ndcg': best_val_ndcg,\n",
    "        'test_ndcg': test_ndcg,\n",
    "        'test_pred': test_pred,\n",
    "        'test_true': test_true,\n",
    "        'test_names': test_names,\n",
    "        'seed': seed\n",
    "    }\n",
    "\n",
    "print(\"✅ Training loop defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda9ae80",
   "metadata": {
    "papermill": {
     "duration": 0.040549,
     "end_time": "2025-11-23T13:07:03.920241",
     "exception": false,
     "start_time": "2025-11-23T13:07:03.879692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f66f716d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:07:04.003280Z",
     "iopub.status.busy": "2025-11-23T13:07:04.002485Z",
     "iopub.status.idle": "2025-11-23T13:11:45.975555Z",
     "shell.execute_reply": "2025-11-23T13:11:45.974733Z"
    },
    "papermill": {
     "duration": 282.015904,
     "end_time": "2025-11-23T13:11:45.976685",
     "exception": false,
     "start_time": "2025-11-23T13:07:03.960781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "V8 BEST MODEL TRAINING (GAT + GRU)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training Best Model (GAT + GRU) with seed 44\n",
      "============================================================\n",
      "Initializing model...\n",
      "Model params: 424,450\n",
      "  Epoch 1: Val NDCG=0.5621\n",
      "  Epoch 2: Val NDCG=0.6151\n",
      "  Epoch 3: Val NDCG=0.5933\n",
      "  Epoch 4: Val NDCG=0.6529\n",
      "  Epoch 5: Val NDCG=0.6611\n",
      "  Epoch 6: Val NDCG=0.6615\n",
      "  Epoch 7: Val NDCG=0.6915\n",
      "  Epoch 8: Val NDCG=0.6898\n",
      "  Epoch 9: Val NDCG=0.6773\n",
      "  Epoch 10: Val NDCG=0.6916\n",
      "  Epoch 11: Val NDCG=0.6736\n",
      "  Epoch 12: Val NDCG=0.6717\n",
      "  Epoch 13: Val NDCG=0.6558\n",
      "  Epoch 14: Val NDCG=0.6709\n",
      "  Epoch 15: Val NDCG=0.6783\n",
      "  Epoch 16: Val NDCG=0.6545\n",
      "  Epoch 17: Val NDCG=0.6740\n",
      "  Epoch 18: Val NDCG=0.6819\n",
      "  Epoch 19: Val NDCG=0.6805\n",
      "  Epoch 20: Val NDCG=0.6805\n",
      "  Epoch 21: Val NDCG=0.6636\n",
      "  Epoch 22: Val NDCG=0.6571\n",
      "  Epoch 23: Val NDCG=0.6656\n",
      "  Epoch 24: Val NDCG=0.6639\n",
      "  Epoch 25: Val NDCG=0.6478\n",
      "  Epoch 26: Val NDCG=0.6747\n",
      "  Epoch 27: Val NDCG=0.6881\n",
      "  Epoch 28: Val NDCG=0.6798\n",
      "  Epoch 29: Val NDCG=0.6570\n",
      "  Epoch 30: Val NDCG=0.6854\n",
      "  Epoch 31: Val NDCG=0.6731\n",
      "  Epoch 32: Val NDCG=0.6989\n",
      "  Epoch 33: Val NDCG=0.6952\n",
      "  Epoch 34: Val NDCG=0.7153\n",
      "  Epoch 35: Val NDCG=0.6740\n",
      "  Epoch 36: Val NDCG=0.6660\n",
      "  Epoch 37: Val NDCG=0.6804\n",
      "  Epoch 38: Val NDCG=0.6800\n",
      "  Epoch 39: Val NDCG=0.6787\n",
      "  Epoch 40: Val NDCG=0.6907\n",
      "  Epoch 41: Val NDCG=0.6773\n",
      "  Epoch 42: Val NDCG=0.6878\n",
      "  Epoch 43: Val NDCG=0.6899\n",
      "  Epoch 44: Val NDCG=0.6750\n",
      "  Epoch 45: Val NDCG=0.6766\n",
      "  Epoch 46: Val NDCG=0.6600\n",
      "  Epoch 47: Val NDCG=0.6766\n",
      "  Epoch 48: Val NDCG=0.6832\n",
      "  Epoch 49: Val NDCG=0.6816\n",
      "  Epoch 50: Val NDCG=0.6801\n",
      "\n",
      "  🏁 Best Val NDCG: 0.7153\n",
      "  🎯 Test NDCG@50: 0.7133\n",
      "\n",
      "============================================================\n",
      "✅ TRAINING COMPLETE in 4.7 minutes\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"V8 BEST MODEL TRAINING (GAT + GRU)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "result = train_single_model(\n",
    "    hetero_graphs,\n",
    "    train_influencers_list, val_influencers_list, test_influencers_list,\n",
    "    all_graphs_data\n",
    ")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ TRAINING COMPLETE in {total_time/60:.1f} minutes\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ee96fa",
   "metadata": {
    "papermill": {
     "duration": 0.042914,
     "end_time": "2025-11-23T13:11:46.063334",
     "exception": false,
     "start_time": "2025-11-23T13:11:46.020420",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ebbc785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:11:46.156875Z",
     "iopub.status.busy": "2025-11-23T13:11:46.156622Z",
     "iopub.status.idle": "2025-11-23T13:11:46.173115Z",
     "shell.execute_reply": "2025-11-23T13:11:46.172408Z"
    },
    "papermill": {
     "duration": 0.06221,
     "end_time": "2025-11-23T13:11:46.174375",
     "exception": false,
     "start_time": "2025-11-23T13:11:46.112165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to saved_models_v8_final/\n"
     ]
    }
   ],
   "source": [
    "# Create save directory\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': result['model'].state_dict(),\n",
    "    'config': {\n",
    "        'INPUT_DIM': INPUT_DIM,\n",
    "        'GNN_HIDDEN': GNN_HIDDEN,\n",
    "        'GNN_OUT': GNN_OUT,\n",
    "        'RNN_HIDDEN': RNN_HIDDEN,\n",
    "        'DROPOUT': DROPOUT,\n",
    "        'STRICT_MASK_INDICES': STRICT_MASK_INDICES,\n",
    "    },\n",
    "    'results': {\n",
    "        'test_ndcg': result['test_ndcg'],\n",
    "        'val_ndcg': result['best_val_ndcg'],\n",
    "        'seed': result['seed'],\n",
    "    }\n",
    "}, f'{MODEL_SAVE_DIR}/model.pt')\n",
    "\n",
    "# Save scaler\n",
    "with open(f'{MODEL_SAVE_DIR}/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save data split\n",
    "with open(f'{MODEL_SAVE_DIR}/data_split.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'train': train_influencers_list,\n",
    "        'val': val_influencers_list,\n",
    "        'test': test_influencers_list,\n",
    "    }, f)\n",
    "\n",
    "print(f\"✅ Model saved to {MODEL_SAVE_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba1ccd7",
   "metadata": {
    "papermill": {
     "duration": 0.049846,
     "end_time": "2025-11-23T13:11:46.275997",
     "exception": false,
     "start_time": "2025-11-23T13:11:46.226151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 12. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ccf41cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:11:46.396771Z",
     "iopub.status.busy": "2025-11-23T13:11:46.396137Z",
     "iopub.status.idle": "2025-11-23T13:11:46.401093Z",
     "shell.execute_reply": "2025-11-23T13:11:46.400373Z"
    },
    "papermill": {
     "duration": 0.055895,
     "end_time": "2025-11-23T13:11:46.402047",
     "exception": false,
     "start_time": "2025-11-23T13:11:46.346152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "  Model: GAT + GRU\n",
      "  Seed: 44\n",
      "  Val NDCG@50: 0.7153\n",
      "  🎯 Test NDCG@50: 0.7133\n",
      "  Gap to paper: 0.0067\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Model: GAT + GRU\")\n",
    "print(f\"  Seed: {result['seed']}\")\n",
    "print(f\"  Val NDCG@50: {result['best_val_ndcg']:.4f}\")\n",
    "\n",
    "print(f\"  🎯 Test NDCG@50: {result['test_ndcg']:.4f}\")\n",
    "print(f\"  Gap to paper: {0.72 - result['test_ndcg']:.4f}\")\n",
    "\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef960679",
   "metadata": {
    "papermill": {
     "duration": 0.04261,
     "end_time": "2025-11-23T13:11:46.488458",
     "exception": false,
     "start_time": "2025-11-23T13:11:46.445848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 13. Top 50 Influencers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "960a3f1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:11:46.574512Z",
     "iopub.status.busy": "2025-11-23T13:11:46.574292Z",
     "iopub.status.idle": "2025-11-23T13:11:46.616953Z",
     "shell.execute_reply": "2025-11-23T13:11:46.616045Z"
    },
    "papermill": {
     "duration": 0.086968,
     "end_time": "2025-11-23T13:11:46.618267",
     "exception": false,
     "start_time": "2025-11-23T13:11:46.531299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 50 INFLUENCERS (Predicted by Model)\n",
      "================================================================================\n",
      "\n",
      " rank            influencer  predicted_score  actual_engagement\n",
      "    1         annakapiteijn         3.938305           0.132770\n",
      "    2          bailee_denny         3.508782           0.098315\n",
      "    3 alexandra.kirchberger         3.406746           0.109142\n",
      "    4               chlo__j         3.321176           0.107476\n",
      "    5        em_ilylonergan         3.249031           0.192504\n",
      "    6           barbspatino         3.087434           0.097757\n",
      "    7           amanda.paul         3.021748           0.094798\n",
      "    8           dianaluchin         2.992497           0.218158\n",
      "    9         brennanfoster         2.723327           0.074667\n",
      "   10          adentrostyle         2.681482           0.086996\n",
      "   11          ellietindall         2.670385           0.099074\n",
      "   12             ellenbuui         2.661301           0.123982\n",
      "   13      ceciliadelacruz7         2.646411           0.146299\n",
      "   14         classiccarlos         2.632510           0.125737\n",
      "   15       christernordnes         2.604949           0.096453\n",
      "   16     caitlinryanharper         2.590474           0.186961\n",
      "   17       beautyandabunny         2.538930           0.059974\n",
      "   18            cammy_ebel         2.388483           0.097936\n",
      "   19             carlykozz         2.384581           0.124760\n",
      "   20        angeliquefiske         2.308944           0.141406\n",
      "   21             dlfactory         2.246596           0.093496\n",
      "   22             beckienye         2.209254           0.083856\n",
      "   23           aliciazitka         2.178046           0.101593\n",
      "   24      andrefbernardino         2.144096           0.101781\n",
      "   25           djsnowtiger         2.129281           0.069368\n",
      "   26         davidhubacher         2.112521           0.118005\n",
      "   27        bitsofbrittany         2.043326           0.034399\n",
      "   28          ellelouisa14         2.015460           0.040341\n",
      "   29        barbarafantini         2.006299           0.088083\n",
      "   30            1nonlycash         2.006214           0.074565\n",
      "   31           ashiepowers         1.997694           0.039916\n",
      "   32      christianarsberg         1.874872           0.050592\n",
      "   33             caramonte         1.797560           0.102982\n",
      "   34          chefjulie_rd         1.775267           0.038498\n",
      "   35            ayehpineda         1.667378           0.056166\n",
      "   36       cassandralrosen         1.661869           0.035891\n",
      "   37            demiplaras         1.657616           0.063794\n",
      "   38             decofixel         1.625826           0.061385\n",
      "   39         celinecortess         1.597273           0.190500\n",
      "   40      dancingdominican         1.574191           0.077296\n",
      "   41              alicorin         1.555444           0.075151\n",
      "   42           babastudios         1.540960           0.089502\n",
      "   43           alinaasirbu         1.511681           0.084010\n",
      "   44        _crazymoments_         1.487171           0.036369\n",
      "   45         antiwheatgirl         1.415650           0.052753\n",
      "   46             _wildwobs         1.412301           0.139321\n",
      "   47      allisoneschuster         1.398296           0.073972\n",
      "   48            aj_breezy_         1.370009           0.312289\n",
      "   49              _moni.j_         1.335017           0.066591\n",
      "   50       danisorianoblog         1.329223           0.030648\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create ranking dataframe\n",
    "ranking_df = pd.DataFrame({\n",
    "    'influencer': result['test_names'],\n",
    "    'predicted_score': result['test_pred'],\n",
    "    'actual_engagement': result['test_true']\n",
    "})\n",
    "\n",
    "# Sort by predicted score\n",
    "ranking_df = ranking_df.sort_values('predicted_score', ascending=False).reset_index(drop=True)\n",
    "ranking_df['rank'] = range(1, len(ranking_df) + 1)\n",
    "\n",
    "# Display top 50\n",
    "top_50 = ranking_df.head(50)[['rank', 'influencer', 'predicted_score', 'actual_engagement']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 50 INFLUENCERS (Predicted by Model)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(top_50.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c21cf",
   "metadata": {
    "papermill": {
     "duration": 0.042442,
     "end_time": "2025-11-23T13:11:46.705425",
     "exception": false,
     "start_time": "2025-11-23T13:11:46.662983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 14. Full Test Set with Rank Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7df31c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:11:46.790888Z",
     "iopub.status.busy": "2025-11-23T13:11:46.790334Z",
     "iopub.status.idle": "2025-11-23T13:11:46.816310Z",
     "shell.execute_reply": "2025-11-23T13:11:46.815482Z"
    },
    "papermill": {
     "duration": 0.070624,
     "end_time": "2025-11-23T13:11:46.818048",
     "exception": false,
     "start_time": "2025-11-23T13:11:46.747424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "FULL TEST SET RANKINGS (474 influencers)\n",
      "==========================================================================================\n",
      "\n",
      " rank                    influencer  predicted_score  actual_engagement rank_class\n",
      "    1                 annakapiteijn         3.938305           0.132770     Top 10\n",
      "    2                  bailee_denny         3.508782           0.098315     Top 10\n",
      "    3         alexandra.kirchberger         3.406746           0.109142     Top 10\n",
      "    4                       chlo__j         3.321176           0.107476     Top 10\n",
      "    5                em_ilylonergan         3.249031           0.192504     Top 10\n",
      "    6                   barbspatino         3.087434           0.097757     Top 10\n",
      "    7                   amanda.paul         3.021748           0.094798     Top 10\n",
      "    8                   dianaluchin         2.992497           0.218158     Top 10\n",
      "    9                 brennanfoster         2.723327           0.074667     Top 10\n",
      "   10                  adentrostyle         2.681482           0.086996     Top 10\n",
      "   11                  ellietindall         2.670385           0.099074     Top 50\n",
      "   12                     ellenbuui         2.661301           0.123982     Top 50\n",
      "   13              ceciliadelacruz7         2.646411           0.146299     Top 50\n",
      "   14                 classiccarlos         2.632510           0.125737     Top 50\n",
      "   15               christernordnes         2.604949           0.096453     Top 50\n",
      "   16             caitlinryanharper         2.590474           0.186961     Top 50\n",
      "   17               beautyandabunny         2.538930           0.059974     Top 50\n",
      "   18                    cammy_ebel         2.388483           0.097936     Top 50\n",
      "   19                     carlykozz         2.384581           0.124760     Top 50\n",
      "   20                angeliquefiske         2.308944           0.141406     Top 50\n",
      "   21                     dlfactory         2.246596           0.093496     Top 50\n",
      "   22                     beckienye         2.209254           0.083856     Top 50\n",
      "   23                   aliciazitka         2.178046           0.101593     Top 50\n",
      "   24              andrefbernardino         2.144096           0.101781     Top 50\n",
      "   25                   djsnowtiger         2.129281           0.069368     Top 50\n",
      "   26                 davidhubacher         2.112521           0.118005     Top 50\n",
      "   27                bitsofbrittany         2.043326           0.034399     Top 50\n",
      "   28                  ellelouisa14         2.015460           0.040341     Top 50\n",
      "   29                barbarafantini         2.006299           0.088083     Top 50\n",
      "   30                    1nonlycash         2.006214           0.074565     Top 50\n",
      "   31                   ashiepowers         1.997694           0.039916     Top 50\n",
      "   32              christianarsberg         1.874872           0.050592     Top 50\n",
      "   33                     caramonte         1.797560           0.102982     Top 50\n",
      "   34                  chefjulie_rd         1.775267           0.038498     Top 50\n",
      "   35                    ayehpineda         1.667378           0.056166     Top 50\n",
      "   36               cassandralrosen         1.661869           0.035891     Top 50\n",
      "   37                    demiplaras         1.657616           0.063794     Top 50\n",
      "   38                     decofixel         1.625826           0.061385     Top 50\n",
      "   39                 celinecortess         1.597273           0.190500     Top 50\n",
      "   40              dancingdominican         1.574191           0.077296     Top 50\n",
      "   41                      alicorin         1.555444           0.075151     Top 50\n",
      "   42                   babastudios         1.540960           0.089502     Top 50\n",
      "   43                   alinaasirbu         1.511681           0.084010     Top 50\n",
      "   44                _crazymoments_         1.487171           0.036369     Top 50\n",
      "   45                 antiwheatgirl         1.415650           0.052753     Top 50\n",
      "   46                     _wildwobs         1.412301           0.139321     Top 50\n",
      "   47              allisoneschuster         1.398296           0.073972     Top 50\n",
      "   48                    aj_breezy_         1.370009           0.312289     Top 50\n",
      "   49                      _moni.j_         1.335017           0.066591     Top 50\n",
      "   50               danisorianoblog         1.329223           0.030648     Top 50\n",
      "   51                  chantrikeele         1.285028           0.060518    Top 100\n",
      "   52                chelseygifford         1.259309           0.057008    Top 100\n",
      "   53                  ellatumbless         1.228666           0.070686    Top 100\n",
      "   54                       bkuss89         1.225570           0.027835    Top 100\n",
      "   55                 courtneylivin         1.157898           0.018599    Top 100\n",
      "   56                  beckyknightx         1.119093           0.042701    Top 100\n",
      "   57                  eellalarsson         1.081719           0.130688    Top 100\n",
      "   58                      casarose         1.064246           0.078642    Top 100\n",
      "   59         edition.makeityourown         1.025407           0.028844    Top 100\n",
      "   60               alejandras_days         1.009920           0.108088    Top 100\n",
      "   61                acarltonmiller         0.986449           0.057764    Top 100\n",
      "   62                  clara_astori         0.974474           0.065461    Top 100\n",
      "   63                claudiapenetra         0.971838           0.020889    Top 100\n",
      "   64                      banshiba         0.968701           0.045871    Top 100\n",
      "   65                escuderocelyne         0.939653           0.055745    Top 100\n",
      "   66                   alyssatabit         0.935171           0.026395    Top 100\n",
      "   67               andreaantequera         0.920169           0.183938    Top 100\n",
      "   68                annemarieamber         0.918823           0.048886    Top 100\n",
      "   69                      _okkins_         0.908140           0.054448    Top 100\n",
      "   70                    aidanomics         0.887273           0.179556    Top 100\n",
      "   71              ___juli_________         0.869737           0.063517    Top 100\n",
      "   72                    ericmunson         0.865922           0.062240    Top 100\n",
      "   73               emilyjayneellis         0.861702           0.056141    Top 100\n",
      "   74                   chavongrace         0.855165           0.035513    Top 100\n",
      "   75            camillefilledavril         0.810208           0.078022    Top 100\n",
      "   76                 christinartnd         0.801223           0.045067    Top 100\n",
      "   77                 ccourtneyhill         0.799342           0.038086    Top 100\n",
      "   78                    codieklein         0.795936           0.043372    Top 100\n",
      "   79                   _mollypayne         0.788555           0.042237    Top 100\n",
      "   80                cristina_ferro         0.783678           0.037303    Top 100\n",
      "   81                  _johnjarrett         0.765261           0.032815    Top 100\n",
      "   82                blakesummerlin         0.748934           0.022677    Top 100\n",
      "   83                colorfulcurves         0.737898           0.018516    Top 100\n",
      "   84                      brixwork         0.709975           0.032544    Top 100\n",
      "   85                   alizeebidou         0.683621           0.061981    Top 100\n",
      "   86              alltimepatsfan24         0.672225           0.042904    Top 100\n",
      "   87                 blettymoreira         0.671364           0.054205    Top 100\n",
      "   88                 clarity_engel         0.642040           0.076081    Top 100\n",
      "   89                   amyydamante         0.634983           0.031726    Top 100\n",
      "   90                  cait_giacino         0.629221           0.054225    Top 100\n",
      "   91               charles_schiele         0.619370           0.082105    Top 100\n",
      "   92                      aurabond         0.592724           0.042532    Top 100\n",
      "   93                      berethar         0.586391           0.067289    Top 100\n",
      "   94                        empemb         0.583221           0.049236    Top 100\n",
      "   95                   elisekvamme         0.572180           0.057520    Top 100\n",
      "   96                     elle_talk         0.569336           0.032811    Top 100\n",
      "   97                carolinagroppa         0.567838           0.012710    Top 100\n",
      "   98              dog_days_of_owen         0.553670           0.031858    Top 100\n",
      "   99         courtneyhoyleofficial         0.536200           0.088317    Top 100\n",
      "  100                    amy.sheree         0.535190           0.022618    Top 100\n",
      "  101         eat.play.sleep.repeat         0.528864           0.017656    Top 200\n",
      "  102            emmanuellepoumirol         0.526974           0.029183    Top 200\n",
      "  103                  andreakelley         0.518480           0.037896    Top 200\n",
      "  104                 britniekaplan         0.518347           0.019674    Top 200\n",
      "  105         destination_whitecoat         0.513433           0.041990    Top 200\n",
      "  106                    emmespivey         0.508162           0.075942    Top 200\n",
      "  107               bobbyschuessler         0.497116           0.024215    Top 200\n",
      "  108                    _joestyles         0.479751           0.027838    Top 200\n",
      "  109                eleventhbeauty         0.479665           0.043455    Top 200\n",
      "  110                  arelivanessa         0.461271           0.048389    Top 200\n",
      "  111              3princesses1dude         0.458389           0.048423    Top 200\n",
      "  112                  alyssiatsang         0.446102           0.113087    Top 200\n",
      "  113                   boltandkeel         0.435309           0.092103    Top 200\n",
      "  114                    bonimakeup         0.429248           0.032434    Top 200\n",
      "  115                      ericluis         0.424651           0.042523    Top 200\n",
      "  116        brooklynthebernedoodle         0.422520           0.028290    Top 200\n",
      "  117          ellenhiddingofficial         0.408219           0.019698    Top 200\n",
      "  118                     _effifit_         0.403270           0.046540    Top 200\n",
      "  119                 alexalbaphoto         0.396535           0.020456    Top 200\n",
      "  120                     arang.ytb         0.396056           0.017663    Top 200\n",
      "  121                        deedls         0.390294           0.040936    Top 200\n",
      "  122                   chxndeliers         0.381211           0.033230    Top 200\n",
      "  123                 charliebrake1         0.375762           0.000377    Top 200\n",
      "  124                 cherry_timato         0.371922           0.088384    Top 200\n",
      "  125                     _sallyjo_         0.361225           0.024355    Top 200\n",
      "  126          authorcarolinegeorge         0.357970           0.043898    Top 200\n",
      "  127                     alwaystri         0.352320           0.025236    Top 200\n",
      "  128                   damn_gina86         0.347733           0.003132    Top 200\n",
      "  129                     cateciabi         0.347459           0.033029    Top 200\n",
      "  130                 amandakenny87         0.347051           0.064484    Top 200\n",
      "  131                 cristinasally         0.346332           0.047272    Top 200\n",
      "  132                    alzatesara         0.344059           0.028839    Top 200\n",
      "  133                  briciaemilyn         0.337336           0.028588    Top 200\n",
      "  134                dannyratcliffe         0.330306           0.107509    Top 200\n",
      "  135                 blacc_velvett         0.318122           0.018241    Top 200\n",
      "  136                     aliolives         0.306332           0.042902    Top 200\n",
      "  137              chrisvalentine17         0.306084           0.008855    Top 200\n",
      "  138                   _elodypetit         0.303477           0.111230    Top 200\n",
      "  139                      danneigh         0.301971           0.023991    Top 200\n",
      "  140                  emokemakszim         0.295286           0.028560    Top 200\n",
      "  141                    dom_mcghee         0.291219           0.046621    Top 200\n",
      "  142                  duhhitsmitch         0.285615           0.066695    Top 200\n",
      "  143                courtz_hancock         0.282982           0.045107    Top 200\n",
      "  144                     acceology         0.280598           0.015936    Top 200\n",
      "  145                    facepoetry         0.275559           0.027215    Top 200\n",
      "  146                   ethantglenn         0.275406           0.028299    Top 200\n",
      "  147             brooklyn_dieterle         0.274743           0.070897    Top 200\n",
      "  148                clairebearmcgr         0.273180           0.024516    Top 200\n",
      "  149                    cath_belle         0.270633           0.029327    Top 200\n",
      "  150             calipso_accesorio         0.268639           0.028073    Top 200\n",
      "  151                       clochet         0.264400           0.005735    Top 200\n",
      "  152                chrismelberger         0.258798           0.042247    Top 200\n",
      "  153                 celinebramsen         0.254212           0.030156    Top 200\n",
      "  154         alleyesongregormcewan         0.232902           0.029216    Top 200\n",
      "  155                  alissaviolet         0.232609           0.103820    Top 200\n",
      "  156                  bobsupersage         0.230231           0.112966    Top 200\n",
      "  157            antonio.gherghetta         0.229377           0.086983    Top 200\n",
      "  158                   1079thelink         0.220208           0.015413    Top 200\n",
      "  159                 agnesgrundahl         0.219809           0.084631    Top 200\n",
      "  160                    deborafini         0.217007           0.136352    Top 200\n",
      "  161             emanuele_bonomini         0.206156           0.033799    Top 200\n",
      "  162                 bakedbyjoanna         0.201985           0.024627    Top 200\n",
      "  163                   atelagaarta         0.196328           0.052635    Top 200\n",
      "  164                     dancelizz         0.187284           0.022175    Top 200\n",
      "  165                 anikacoutinho         0.183904           0.014782    Top 200\n",
      "  166                  colinlevitch         0.176116           0.014056    Top 200\n",
      "  167                    chazzdowns         0.174472           0.121522    Top 200\n",
      "  168            adcriaepersonaliza         0.169175           0.028314    Top 200\n",
      "  169                breezysmamager         0.163764           0.022973    Top 200\n",
      "  170                      cazloves         0.150643           0.030541    Top 200\n",
      "  171                  aminaali_mua         0.148911           0.064399    Top 200\n",
      "  172                   ayaka__0427         0.145852           0.040967    Top 200\n",
      "  173              camille_brunelle         0.131318           0.027033    Top 200\n",
      "  174                    eshavantha         0.121292           0.021457    Top 200\n",
      "  175                  caro_manning         0.103261           0.056266    Top 200\n",
      "  176                    bixbyfilms         0.093473           0.014527    Top 200\n",
      "  177                       amysoub         0.092685           0.139563    Top 200\n",
      "  178               ateliersjouffre         0.087943           0.072466    Top 200\n",
      "  179                   cnnlifelive         0.072827           0.015432    Top 200\n",
      "  180           angeliqueemmanuelle         0.069805           0.036708    Top 200\n",
      "  181              evinalepotilnica         0.069624           0.059318    Top 200\n",
      "  182                 ayla_woodruff         0.058381           0.086372    Top 200\n",
      "  183                  byrontalbott         0.052370           0.023423    Top 200\n",
      "  184        erika_prihodova_artist         0.052180           0.052896    Top 200\n",
      "  185                  bowl_me_over         0.050905           0.070543    Top 200\n",
      "  186               craftandcouture         0.041651           0.021599    Top 200\n",
      "  187                      bymersel         0.041226           0.036020    Top 200\n",
      "  188                   biancaebako         0.037914           0.012025    Top 200\n",
      "  189                 courteney_mua         0.031507           0.034315    Top 200\n",
      "  190               champagnechores         0.028240           0.042798    Top 200\n",
      "  191              dilettagomezgane         0.022129           0.022825    Top 200\n",
      "  192                      ari_says         0.021024           0.007772    Top 200\n",
      "  193                alenapolackova         0.020780           0.085302    Top 200\n",
      "  194                  fiamapereira         0.017298           0.023175    Top 200\n",
      "  195               dirtybombshells         0.014962           0.006095    Top 200\n",
      "  196                   danielarued         0.011222           0.072834    Top 200\n",
      "  197                      akaydoll         0.011104           0.035055    Top 200\n",
      "  198                  faiithmarone         0.010710           0.040136    Top 200\n",
      "  199                  chriswackert         0.004110           0.026768    Top 200\n",
      "  200                ericaonfashion        -0.000857           0.022880    Top 200\n",
      "  201              alexandras.light        -0.003011           0.112978      Other\n",
      "  202                  ericasworld_        -0.008428           0.028706      Other\n",
      "  203                     ammarhina        -0.009424           0.056653      Other\n",
      "  204                   codyondrick        -0.013859           0.047174      Other\n",
      "  205        erikathevikingvikander        -0.024772           0.033951      Other\n",
      "  206               beautiful.tribe        -0.031160           0.034958      Other\n",
      "  207                    emilybegin        -0.031161           0.022337      Other\n",
      "  208                  docdocdoc.ru        -0.045741           0.038699      Other\n",
      "  209            fabivongerneschoen        -0.046284           0.038991      Other\n",
      "  210                       flabgee        -0.059324           0.063930      Other\n",
      "  211                 dezemberliebe        -0.059434           0.115482      Other\n",
      "  212                belrodrigues13        -0.059918           0.027516      Other\n",
      "  213                    crisshex88        -0.063299           0.017058      Other\n",
      "  214                  ainasyahirah        -0.068727           0.020362      Other\n",
      "  215            emelgloss_official        -0.072254           0.023507      Other\n",
      "  216                  bienestar.co        -0.084556           0.014296      Other\n",
      "  217      columbiajournalismreview        -0.085992           0.018791      Other\n",
      "  218                    aminamarie        -0.086703           0.038083      Other\n",
      "  219                  emmavanstone        -0.089573           0.013855      Other\n",
      "  220              drcristina_dubai        -0.091805           0.005683      Other\n",
      "  221              charlinecatteeuw        -0.092189           0.022518      Other\n",
      "  222                       aaliy0h        -0.093368           0.041933      Other\n",
      "  223                babbleonbrooke        -0.096493           0.037075      Other\n",
      "  224                        domien        -0.098467           0.035719      Other\n",
      "  225                  adriancanolo        -0.102059           0.009054      Other\n",
      "  226                  carlafitlife        -0.102263           0.014734      Other\n",
      "  227                cheetara_flame        -0.110026           0.005967      Other\n",
      "  228                    bethrodden        -0.116154           0.039073      Other\n",
      "  229                    ericoborgo        -0.118534           0.051213      Other\n",
      "  230                     bergstrvm        -0.119100           0.069186      Other\n",
      "  231                catherine.wigg        -0.133361           0.048679      Other\n",
      "  232                blackmogulsmag        -0.135925           0.005896      Other\n",
      "  233                     alexx_xox        -0.148901           0.048268      Other\n",
      "  234                 enicoreblimey        -0.150756           0.043565      Other\n",
      "  235                   anaffnandez        -0.153416           0.045511      Other\n",
      "  236          everydayallergenfree        -0.156237           0.009609      Other\n",
      "  237                andreaalonsoaa        -0.156426           0.067241      Other\n",
      "  238                alexanderson29        -0.160124           0.027675      Other\n",
      "  239                  dianawandall        -0.164537           0.041996      Other\n",
      "  240                  astheygrowup        -0.165442           0.008630      Other\n",
      "  241                     adriehawk        -0.166872           0.037326      Other\n",
      "  242                   anasegui333        -0.172903           0.012962      Other\n",
      "  243                   baumanelise        -0.173781           0.095828      Other\n",
      "  244               charlottesandal        -0.175353           0.042217      Other\n",
      "  245                         aylak        -0.183338           0.065644      Other\n",
      "  246                     amaldaliz        -0.184447           0.022677      Other\n",
      "  247                 chrishogan_15        -0.186645           0.074208      Other\n",
      "  248                          alia        -0.187817           0.020096      Other\n",
      "  249                      eyasghir        -0.188167           0.015751      Other\n",
      "  250                     danakadan        -0.195718           0.034567      Other\n",
      "  251                    douniaerts        -0.199285           0.024901      Other\n",
      "  252               christophs.welt        -0.201202           0.104689      Other\n",
      "  253                eminence.onesi        -0.204713           0.041514      Other\n",
      "  254                    carakilbey        -0.204847           0.012997      Other\n",
      "  255                bridget.dowden        -0.208411           0.016902      Other\n",
      "  256                     cestvrai_        -0.209678           0.016579      Other\n",
      "  257                 amitkarpe_adk        -0.215239           0.022112      Other\n",
      "  258            courtneybpinkerton        -0.216239           0.042254      Other\n",
      "  259                   curlyjojo__        -0.218090           0.016937      Other\n",
      "  260               followaflamingo        -0.220046           0.119322      Other\n",
      "  261               bakingheartblog        -0.221190           0.021540      Other\n",
      "  262               caroleradziwill        -0.223195           0.019020      Other\n",
      "  263                 alvarodigital        -0.224289           0.066002      Other\n",
      "  264                 deliriumstyle        -0.228188           0.009636      Other\n",
      "  265                   elinwilkens        -0.229235           0.020434      Other\n",
      "  266               claireminawhite        -0.231981           0.019658      Other\n",
      "  267                  alejavilleta        -0.232173           0.043896      Other\n",
      "  268                    _gol.goli_        -0.235876           0.042634      Other\n",
      "  269           claudiosossaoficial        -0.250394           0.017477      Other\n",
      "  270              aleksanderflight        -0.250958           0.082681      Other\n",
      "  271              einarthefrenchie        -0.256765           0.043737      Other\n",
      "  272                 camerontewson        -0.259348           0.019595      Other\n",
      "  273                   danielbetan        -0.262890           0.015511      Other\n",
      "  274                cashleyfishing        -0.263497           0.022031      Other\n",
      "  275                  attagirlsays        -0.265646           0.016716      Other\n",
      "  276                        cam_ds        -0.274451           0.032419      Other\n",
      "  277                   cjjohnsonjr        -0.280315           0.031312      Other\n",
      "  278               firstround.pick        -0.281824           0.040917      Other\n",
      "  279                        dedo90        -0.282589           0.032064      Other\n",
      "  280                 davidbugenske        -0.291766           0.077576      Other\n",
      "  281                  fashionfundi        -0.293160           0.020994      Other\n",
      "  282                   charliem015        -0.295728           0.021730      Other\n",
      "  283                    cindylimon        -0.301158           0.015468      Other\n",
      "  284             aretastylesecrets        -0.305752           0.025231      Other\n",
      "  285                     bad_birdy        -0.306769           0.022723      Other\n",
      "  286                     caseyhl91        -0.311797           0.070990      Other\n",
      "  287              anouskaanastasia        -0.312015           0.032802      Other\n",
      "  288               aurelie.youtube        -0.319669           0.051941      Other\n",
      "  289                     darbyward        -0.333275           0.022707      Other\n",
      "  290                       el.vino        -0.333319           0.023788      Other\n",
      "  291                      chousner        -0.336187           0.021267      Other\n",
      "  292               earlgrey_thecat        -0.336277           0.042743      Other\n",
      "  293                 bambambaklava        -0.343458           0.018673      Other\n",
      "  294                   ashleyncote        -0.347819           0.062797      Other\n",
      "  295                 darrenhewitt_        -0.348357           0.021591      Other\n",
      "  296                    carenshope        -0.349259           0.061849      Other\n",
      "  297           darya_zagranichnaya        -0.354440           0.036143      Other\n",
      "  298                elizabethluiss        -0.360197           0.019474      Other\n",
      "  299             chillingwithlucas        -0.363070           0.027013      Other\n",
      "  300                     elle_loha        -0.365517           0.029712      Other\n",
      "  301                busygirlhealth        -0.370080           0.036827      Other\n",
      "  302             dodge_challengers        -0.371245           0.043755      Other\n",
      "  303               carmensegattini        -0.378084           0.059409      Other\n",
      "  304                    coachbarbs        -0.386241           0.019051      Other\n",
      "  305           comarcadoparahybuna        -0.388200           0.024683      Other\n",
      "  306                     cayir1903        -0.391266           0.020321      Other\n",
      "  307                  abekislevitz        -0.394057           0.037298      Other\n",
      "  308                     eff.ulloa        -0.398131           0.010351      Other\n",
      "  309              amandacaseyvance        -0.406835           0.031456      Other\n",
      "  310                 allisontuttle        -0.408806           0.054173      Other\n",
      "  311                       ckaylim        -0.412689           0.001282      Other\n",
      "  312                alexgracejones        -0.417673           0.108123      Other\n",
      "  313                    annimarikk        -0.429168           0.039390      Other\n",
      "  314                       byokids        -0.435840           0.019706      Other\n",
      "  315                   ashrachelle        -0.446065           0.022908      Other\n",
      "  316              angelicamasturzo        -0.448231           0.018210      Other\n",
      "  317                darlingdearest        -0.448373           0.024602      Other\n",
      "  318                   adelicia410        -0.457147           0.009720      Other\n",
      "  319                 _pommegranate        -0.457394           0.027697      Other\n",
      "  320                 alancarinocom        -0.458691           0.017744      Other\n",
      "  321                fitmomma4three        -0.464685           0.010620      Other\n",
      "  322               anastasiabsmith        -0.472031           0.046410      Other\n",
      "  323                dachshund_nola        -0.487605           0.052552      Other\n",
      "  324            emilyobrienstylist        -0.491415           0.035184      Other\n",
      "  325               avantimorocha_1        -0.491639           0.006824      Other\n",
      "  326                  bahgutierrez        -0.494363           0.056632      Other\n",
      "  327                   domanishoes        -0.497593           0.008454      Other\n",
      "  328                 carolbeckhaam        -0.506000           0.024551      Other\n",
      "  329                    charity216        -0.508529           0.027927      Other\n",
      "  330               chloecrowhurstx        -0.510634           0.030535      Other\n",
      "  331                   fauziaradja        -0.511075           0.037745      Other\n",
      "  332                   aureliablog        -0.513614           0.017212      Other\n",
      "  333                    di.anajune        -0.520728           0.028208      Other\n",
      "  334              chasing_supermom        -0.521061           0.009195      Other\n",
      "  335                    catanoglam        -0.526742           0.019710      Other\n",
      "  336               bgorgeousguurrl        -0.528355           0.037822      Other\n",
      "  337                     analago95        -0.534248           0.041280      Other\n",
      "  338                     flybyeats        -0.534413           0.069101      Other\n",
      "  339               _davidecorsini_        -0.536285           0.032262      Other\n",
      "  340                  dorine_tovim        -0.536326           0.024510      Other\n",
      "  341           fitfreedomlifestyle        -0.544785           0.068653      Other\n",
      "  342                 blackbiirdfly        -0.557569           0.034652      Other\n",
      "  343                christinalichi        -0.559020           0.029891      Other\n",
      "  344                   estherjulee        -0.561182           0.022917      Other\n",
      "  345                   denver_eats        -0.564745           0.012844      Other\n",
      "  346                  cocoaandsalt        -0.577695           0.016930      Other\n",
      "  347                 cook_eat_love        -0.610658           0.010479      Other\n",
      "  348                   ellena_gene        -0.616367           0.011732      Other\n",
      "  349                 charlottehuco        -0.617919           0.011564      Other\n",
      "  350             _thegypsygoddess_        -0.626676           0.051745      Other\n",
      "  351                    anime_goth        -0.627206           0.043307      Other\n",
      "  352                      flafavro        -0.630021           0.026889      Other\n",
      "  353             angel_inspiration        -0.635111           0.036425      Other\n",
      "  354                annakendrickvn        -0.636137           0.015175      Other\n",
      "  355                      edeena.k        -0.639608           0.098571      Other\n",
      "  356                  darlanncosta        -0.643025           0.019819      Other\n",
      "  357               diva_with_favur        -0.645353           0.034328      Other\n",
      "  358               amyclaireblaker        -0.645792           0.022991      Other\n",
      "  359                   camille_mbl        -0.649583           0.037244      Other\n",
      "  360                    camsbotten        -0.652198           0.094668      Other\n",
      "  361                  _nina_garcia        -0.655229           0.008796      Other\n",
      "  362                 anllela_sagra        -0.662291           0.020377      Other\n",
      "  363             cookingwithjanica        -0.664223           0.033801      Other\n",
      "  364                     claudelhy        -0.684606           0.068291      Other\n",
      "  365                      dielenka        -0.684988           0.024765      Other\n",
      "  366                   aikocunanan        -0.696994           0.014141      Other\n",
      "  367                    chillibean        -0.697464           0.017517      Other\n",
      "  368                      badxmami        -0.702309           0.007366      Other\n",
      "  369                 andreabassano        -0.703893           0.029041      Other\n",
      "  370                  casa_colibri        -0.722391           0.021470      Other\n",
      "  371                 catalinamorac        -0.725725           0.009995      Other\n",
      "  372                   drewftaylor        -0.730380           0.039067      Other\n",
      "  373                      dr.nickc        -0.731476           0.022170      Other\n",
      "  374            eleonoreandmaurice        -0.733441           0.019722      Other\n",
      "  375                  emiliajmusic        -0.734590           0.007670      Other\n",
      "  376                   atpcreative        -0.736508           0.006583      Other\n",
      "  377                angeliquemiles        -0.738714           0.024863      Other\n",
      "  378            alittlebitoflottie        -0.754674           0.014692      Other\n",
      "  379                  feedmedearly        -0.761604           0.023899      Other\n",
      "  380               cowboyslifeblog        -0.762682           0.007503      Other\n",
      "  381                 alexismahdavi        -0.769015           0.020510      Other\n",
      "  382                  ashleyspivey        -0.774890           0.015325      Other\n",
      "  383                _hannahsilver_        -0.789624           0.008829      Other\n",
      "  384                 erikamariemua        -0.790482           0.051440      Other\n",
      "  385                   cortneyhare        -0.814807           0.049657      Other\n",
      "  386              alessandro_garza        -0.820263           0.013366      Other\n",
      "  387                 elenaschiavon        -0.824644           0.014327      Other\n",
      "  388                   alphafoodie        -0.826900           0.029901      Other\n",
      "  389                   alexaschirm        -0.829838           0.019580      Other\n",
      "  390             everydayabovedirt        -0.833484           0.045257      Other\n",
      "  391               erikakosegarten        -0.834460           0.041987      Other\n",
      "  392                    bu_saleh10        -0.843670           0.061183      Other\n",
      "  393                       bydudag        -0.844650           0.047999      Other\n",
      "  394                 alexsimpson_x        -0.850314           0.019650      Other\n",
      "  395                beatriceturner        -0.850602           0.016300      Other\n",
      "  396                   a1delatorre        -0.852844           0.012983      Other\n",
      "  397         and_then_there_were_6        -0.853825           0.027840      Other\n",
      "  398                 foggoffashion        -0.867324           0.065481      Other\n",
      "  399                  anna.scrigni        -0.883541           0.101575      Other\n",
      "  400               carmen_recupito        -0.885320           0.011312      Other\n",
      "  401                      flyawhey        -0.888599           0.022160      Other\n",
      "  402               featherdownsoul        -0.907921           0.027397      Other\n",
      "  403                  almosthemoon        -0.918916           0.011599      Other\n",
      "  404               danslelakehouse        -0.924677           0.007806      Other\n",
      "  405                  chris_voyage        -0.943899           0.044856      Other\n",
      "  406                    artist_one        -0.944624           0.001562      Other\n",
      "  407                 ciararaemusic        -0.955416           0.023699      Other\n",
      "  408             colleengallagher_        -0.955607           0.021569      Other\n",
      "  409                  allsaintseve        -0.966296           0.017800      Other\n",
      "  410           fedebyfede_official        -0.966370           0.017998      Other\n",
      "  411                     chubert00        -0.968084           0.022625      Other\n",
      "  412                       eslimah        -0.992138           0.023761      Other\n",
      "  413                  bravesoul.co        -0.997223           0.017074      Other\n",
      "  414                  daquanawhite        -1.014010           0.024736      Other\n",
      "  415                  fannyyockell        -1.017729           0.052997      Other\n",
      "  416                      flavcity        -1.021816           0.005301      Other\n",
      "  417                     camilaolf        -1.037588           0.034705      Other\n",
      "  418                alexa_fan_club        -1.040829           0.039948      Other\n",
      "  419                    annlestyle        -1.046870           0.011278      Other\n",
      "  420                  amy_greaves1        -1.053443           0.023404      Other\n",
      "  421                 andrearenee00        -1.055270           0.017884      Other\n",
      "  422                  andreafeczko        -1.064150           0.013195      Other\n",
      "  423               derrickdowneyjr        -1.067841           0.023179      Other\n",
      "  424                     co_mtnmom        -1.077666           0.032755      Other\n",
      "  425                      barbikav        -1.083831           0.026964      Other\n",
      "  426               brooklynkitten_        -1.094157           0.017752      Other\n",
      "  427                 curvygirlchic        -1.104015           0.011314      Other\n",
      "  428                  defywithdena        -1.128532           0.020013      Other\n",
      "  429                christinkubsch        -1.160305           0.005476      Other\n",
      "  430                 djhappycolors        -1.170764           0.036342      Other\n",
      "  431                   fitluvinmom        -1.184724           0.009995      Other\n",
      "  432             bucketlistjourney        -1.192480           0.010572      Other\n",
      "  433             ericethridgemusic        -1.228465           0.026251      Other\n",
      "  434                    casagrella        -1.241690           0.011893      Other\n",
      "  435            christiandawdesign        -1.255138           0.024836      Other\n",
      "  436           beautifullyhealthy_        -1.264661           0.015852      Other\n",
      "  437           brokemillennialblog        -1.273971           0.019287      Other\n",
      "  438                   datfoodporn        -1.319434           0.025951      Other\n",
      "  439             anakarinavillalba        -1.321105           0.004196      Other\n",
      "  440                 chicsophistic        -1.330183           0.006082      Other\n",
      "  441                dominiquebinns        -1.330272           0.017425      Other\n",
      "  442            chocolatenchildren        -1.330276           0.011955      Other\n",
      "  443                    ericaswalk        -1.340837           0.005844      Other\n",
      "  444                   ffionsnaith        -1.342809           0.052367      Other\n",
      "  445       confessionsofafitfoodie        -1.342834           0.008265      Other\n",
      "  446                    beauxgeaux        -1.364963           0.033160      Other\n",
      "  447                  angesdesucre        -1.380140           0.014384      Other\n",
      "  448                 arlothepomsky        -1.380219           0.032147      Other\n",
      "  449             ashleykaylamakeup        -1.388690           0.010740      Other\n",
      "  450                  chelseakrost        -1.399598           0.014330      Other\n",
      "  451                  cherein.cook        -1.403949           0.016069      Other\n",
      "  452               biscuitsnmakeup        -1.409401           0.008254      Other\n",
      "  453                fabulousity.it        -1.413218           0.048878      Other\n",
      "  454                  danielleomar        -1.416299           0.012127      Other\n",
      "  455                     fabios_87        -1.416696           0.026521      Other\n",
      "  456                busymomshelper        -1.417582           0.008945      Other\n",
      "  457         backpackerswanderlust        -1.448595           0.022482      Other\n",
      "  458                    ayuccchiii        -1.481164           0.026246      Other\n",
      "  459                  cupcakebands        -1.483180           0.013578      Other\n",
      "  460               bellesserestyle        -1.498512           0.009428      Other\n",
      "  461              diyinspiredhouse        -1.535101           0.016994      Other\n",
      "  462                  _jackfowler_        -1.594408           0.000697      Other\n",
      "  463              beautyandthevlog        -1.617229           0.013660      Other\n",
      "  464                    _berrykid_        -1.626908           0.058453      Other\n",
      "  465                   blackrose45        -1.645792           0.000771      Other\n",
      "  466                devinalexander        -1.670411           0.006639      Other\n",
      "  467               dianehoffmaster        -1.706771           0.036877      Other\n",
      "  468             chefstuartokeeffe        -1.723765           0.017089      Other\n",
      "  469                     diy_candy        -1.755215           0.009906      Other\n",
      "  470            crayonsandcravings        -1.787955           0.053098      Other\n",
      "  471 eudoracosmeticosprontaentrega        -1.859569           0.004451      Other\n",
      "  472             fiercebeyondforty        -1.918461           0.056303      Other\n",
      "  473                  bullocksbuzz        -1.946287           0.012007      Other\n",
      "  474                   casamoncada        -2.067669           0.008424      Other\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "Rank Class Distribution:\n",
      "rank_class\n",
      "Other      274\n",
      "Top 10      10\n",
      "Top 100     50\n",
      "Top 200    100\n",
      "Top 50      40\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Add rank class\n",
    "def get_rank_class(rank):\n",
    "    if rank <= 10:\n",
    "        return 'Top 10'\n",
    "    elif rank <= 50:\n",
    "        return 'Top 50'\n",
    "    elif rank <= 100:\n",
    "        return 'Top 100'\n",
    "    elif rank <= 200:\n",
    "        return 'Top 200'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "ranking_df['rank_class'] = ranking_df['rank'].apply(get_rank_class)\n",
    "\n",
    "# Full ranking\n",
    "full_ranking = ranking_df[['rank', 'influencer', 'predicted_score', 'actual_engagement', 'rank_class']]\n",
    "\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"FULL TEST SET RANKINGS ({len(full_ranking)} influencers)\")\n",
    "print(f\"{'='*90}\\n\")\n",
    "print(full_ranking.to_string(index=False))\n",
    "print(f\"\\n{'='*90}\")\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nRank Class Distribution:\")\n",
    "print(full_ranking['rank_class'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8763045,
     "sourceId": 13768913,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 511.828821,
   "end_time": "2025-11-23T13:11:48.384353",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-23T13:03:16.555532",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
